{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gain_modulation_2denselayers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isapome/GainModulation/blob/main/Gain_modulation_2denselayers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxHB1Az0Bsut"
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0tKH_rzBu6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f84b248c-1bc1-4b69-932d-9fe88d486967"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-87bd67be-9af1-ce83-ac58-9a9caf3bfebe)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ0_HcqrBwnf"
      },
      "source": [
        "# Abstract layer class, do not initialize\n",
        "class Layer:\n",
        "    def __init__(self, shape_in, shape_out, s, w_std=0.02, activation=tf.identity, dropout_rate=0., flatten=False):\n",
        "        # Arguments:\n",
        "          # shape_in, shape_out := lists containing dimensions of layer input and output\n",
        "          # s, w_std := weight initialization seed (integer) and standard deviation (float)\n",
        "        if isinstance(shape_in, list):\n",
        "          self.shape_in = shape_in\n",
        "        else:\n",
        "          self.shape_in = list(shape_in)\n",
        "        self.shape_out = shape_out\n",
        "\n",
        "        # Weights are sampled from a normal distribution, bias and attentional gain are initialized to 0\n",
        "        self.ff_weights = tf.Variable(tf.random.normal(self.get_weights(), stddev=w_std, seed=s))\n",
        "        self.bias = tf.Variable(tf.zeros(shape_out, dtype=tf.float32))\n",
        "        self.attention = tf.Variable(tf.zeros(shape_out, dtype=tf.float64))\n",
        "\n",
        "        if flatten is True:\n",
        "          self.flatten = flatten\n",
        "        else:\n",
        "          self.flatten = np.shape(shape_in) > np.shape(shape_out)\n",
        "\n",
        "        self.activation = activation\n",
        "        self.dropout_rate, self.mask = dropout_rate, 0. #is self.mask the seed for a dropout of a forward pass? Is it the same for each layer? \n",
        "        #Does it mean that all the dropout layers will have the same mask in one go?\n",
        "        self.layer_in, self.layer_out, self.cum_input = None, None, None\n",
        "\n",
        "    # General forward pass operation to compute layer output\n",
        "    def forward_pass(self, inputs, training_phase):\n",
        "        shape_in = inputs.shape[1:]\n",
        "\n",
        "        if shape_in != self.shape_in:\n",
        "            raise Exception('Wrong input size: {}, required input: {}'.format(shape_in, self.shape_in))\n",
        "            \n",
        "        if self.flatten:\n",
        "            inputs = tf.reshape(inputs, [-1, tf.reduce_prod(shape_in)])\n",
        "\n",
        "        # Implementation in subclass (Dense, Conv, ...), returns layer-specific input\n",
        "        self.layer_in, layer_out = self.get_ff_output(inputs)\n",
        "        layer_out = tf.add(layer_out, self.bias)\n",
        "        \n",
        "        # Cumulative input, corresponds to y_in in report\n",
        "        self.cum_input = tf.cast(layer_out, tf.float64)\n",
        "        \n",
        "        # Perform operation using 64 bits to prevent rounding errors (why? makes everything slower and is it really necessary to have 15 instead of 6 precision? \n",
        "        #Then why don't you use the same in APPROX? line 37 here vs line 33 in APPROX) \n",
        "        # Rounding errors not necessary in APP\n",
        "        att_gain = tf.cast(tf.multiply(self.cum_input, self.attention), tf.float32)\n",
        "        layer_out = self.activation(tf.add(layer_out, att_gain))\n",
        "\n",
        "        if self.dropout_rate > 0 and training_phase:\n",
        "          layer_out = tf.nn.dropout(layer_out, self.dropout_rate, seed=self.mask)\n",
        "\n",
        "        self.layer_out = layer_out\n",
        "        return layer_out\n",
        "\n",
        "    # General backward pass to update attention and compute feedback\n",
        "    def backward_pass(self, prev_feedback):\n",
        "\n",
        "        # Activation function derivative over feedback\n",
        "        gated_feedback = self.feedback_gating(prev_feedback)\n",
        "\n",
        "        # Implementation in subclass, returns layer-specific feedback\n",
        "        curr_feedback = self.get_fb_output(gated_feedback)\n",
        "\n",
        "        # Compute attention update, cap attentional gain at 50%\n",
        "\n",
        "        ####is this purely for bio reasons? Different caps not tested yet. In bio 10% gain on average, 50 is a bit too large\n",
        "\n",
        "        delta_att = tf.reduce_sum(tf.multiply(tf.cast(gated_feedback, tf.float64), self.cum_input), axis=0) #how you change the betas\n",
        "        self.attention.assign(tf.clip_by_value(tf.add(self.attention, delta_att), -0.5, 0.5))\n",
        "\n",
        "        if self.flatten:\n",
        "            curr_feedback = tf.reshape(curr_feedback, [-1] + self.shape_in)\n",
        "\n",
        "        return curr_feedback\n",
        "\n",
        "    def weight_update(self, rpe):\n",
        "        attended_rpe = tf.multiply(tf.cast(rpe, tf.float64), self.attention)\n",
        "\n",
        "        # Implementation in subclass, returns weight and bias update according to HEB\n",
        "        delta_bias, delta_weights = self.get_weight_update(attended_rpe)\n",
        "\n",
        "        # Reset attention and mask\n",
        "        self.attention.assign(tf.zeros(self.shape_out, dtype=tf.float64))\n",
        "        self.mask += 1. #???????\n",
        "\n",
        "        self.bias.assign_add(delta_bias)\n",
        "        self.ff_weights.assign_add(delta_weights)\n",
        "\n",
        "    # Computes activation function derivative over feedback\n",
        "    def feedback_gating(self, feedback):\n",
        "        if self.activation == tf.nn.relu:\n",
        "            zeros = tf.zeros_like(self.layer_out)\n",
        "            return tf.where(tf.greater(self.layer_out, zeros), feedback, zeros)\n",
        "        elif self.activation == tf.nn.softmax or self.activation == tf.nn.sigmoid:  #why? in the output layer do we still update only one set of weights?\n",
        "            return tf.multiply(tf.multiply(self.layer_out, 1 - self.layer_out), feedback)\n",
        "        elif self.activation == tf.math.tanh:\n",
        "            return tf.multiply(tf.multiply(1 + self.layer_out, 1 - self.layer_out), feedback)\n",
        "        else:\n",
        "            return feedback"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rLQpijRBwuz"
      },
      "source": [
        "# Fully connected layer, see Layer descriptions\n",
        "class Dense(Layer):\n",
        "    def get_weights(self):\n",
        "        return [tf.reduce_prod(self.shape_in), self.shape_out[0]]\n",
        "\n",
        "    def get_ff_output(self, inputs):\n",
        "        return inputs, tf.matmul(inputs, self.ff_weights)\n",
        "\n",
        "    def get_fb_output(self, feedback):\n",
        "        delta_weights = self.get_weight_update(self.attention)[1]\n",
        "        new_weights = tf.add(delta_weights, self.ff_weights)\n",
        "        return tf.matmul(feedback, tf.transpose(new_weights)) #can just return new_weights and do the matmul outside:)\n",
        "    \n",
        "    def get_weight_update(self, attention):\n",
        "        norm_fb = tf.math.divide_no_nan(attention, self.cum_input)\n",
        "        norm_fb = tf.cast(norm_fb, tf.float32)\n",
        "        return tf.reduce_sum(norm_fb, axis=0), tf.matmul(tf.transpose(self.layer_in), norm_fb)\n",
        "\n",
        "# Convolutional layer\n",
        "class Conv(Layer):\n",
        "    def __init__(self, shape_in, filters, s, padding, stride, w_std=0.02, activation=tf.nn.relu, dropout_rate=0.):\n",
        "        # Arguments:\n",
        "          # shape_in, filters := lists containing dimensions of layer input and filter width, height and count\n",
        "          # s, w_std := weight initialization seed (integer) and standard deviation (float)\n",
        "          # padding := string 'VALID' or 'SAME'\n",
        "          # stride := list containing the horizonal and vertical step size\n",
        "\n",
        "        # Number of weights equals: (width filters * length filters * depth of previous layer) over the number of filters\n",
        "        self.get_weights = lambda : [int(filters[0] * filters[1] * shape_in[2]), filters[2]]\n",
        "\n",
        "        # Returns a 4-dimensional Tensor containing patches from the input layer\n",
        "        self.get_patches = lambda x: tf.image.extract_patches(x, [1, filters[0], filters[1], 1],\n",
        "                                                              [1, stride[0], stride[1], 1], [1, 1, 1, 1], padding)\n",
        "        self.get_patches_inv = None\n",
        "\n",
        "        # Initialize remaining parameters\n",
        "        n_patches = self.get_n_patches(filters[:2], padding, stride, shape_in[:2])\n",
        "        super().__init__(shape_in, n_patches + filters[-1:], s, w_std, activation, dropout_rate)\n",
        "\n",
        "    def get_n_patches(self, filters, padding, stride, shape_in):\n",
        "        if padding == 'SAME':  # the size of output is the same as the input when stride=1\n",
        "            return [int(tf.math.ceil(x / y)) for (x, y) in zip(shape_in, stride)]\n",
        "        elif padding == 'VALID':  # no padding\n",
        "            return [(x - y) // z + 1 for (x, y, z) in zip(shape_in, filters, stride)]\n",
        "        else:\n",
        "            raise Exception(\"Padding unknown\")\n",
        "\n",
        "    def get_ff_output(self, inputs):\n",
        "\n",
        "        # If different sized baches, set to -> if True:\n",
        "        if self.get_patches_inv is None: # compute inverse of get_patches\n",
        "          with tf.GradientTape(persistent=True) as t:\n",
        "              t.watch(inputs)\n",
        "              patches = self.get_patches(inputs)\n",
        "\n",
        "          # Number of patches per input node\n",
        "          num_patches = t.gradient(patches, inputs)[0]\n",
        "          self.get_patches_inv = lambda fb: t.gradient(patches, inputs, output_gradients=fb) / num_patches\n",
        "        else:\n",
        "          patches = self.get_patches(inputs)\n",
        "        return patches, tf.einsum('abcd, de -> abce', patches, self.ff_weights)\n",
        "\n",
        "    def get_fb_output(self, feedback):\n",
        "        delta_weights = self.get_weight_update(self.attention)[1]\n",
        "        new_weights = tf.add(delta_weights, self.ff_weights)\n",
        "        curr_feedback = tf.einsum('abcd, de -> abce', feedback, tf.transpose(new_weights))\n",
        "        curr_feedback = self.get_patches_inv(curr_feedback)\n",
        "        return tf.where(tf.math.is_nan(curr_feedback), tf.zeros_like(curr_feedback), curr_feedback)\n",
        "\n",
        "    def get_weight_update(self, attention):\n",
        "        norm_fb = tf.math.divide_no_nan(attention, self.cum_input)\n",
        "        norm_fb = tf.cast(norm_fb, tf.float32)\n",
        "        return tf.reduce_sum(norm_fb, axis=0), tf.einsum('abcd,abce->de', self.layer_in, norm_fb)\n",
        "\n",
        "\n",
        "# Layer initialization functions\n",
        "def conv(filters, padding, stride, w_std=0.02, activation=tf.nn.relu, dropout_rate=0.):\n",
        "    return lambda x, y: Conv(x, filters, y, padding, stride, w_std, activation, dropout_rate)\n",
        "\n",
        "def dense(size_layer, w_std=0.02, activation=tf.nn.relu, dropout_rate=0., flatten=False):\n",
        "    return lambda x, y: Dense(x, [size_layer], y, w_std, activation, dropout_rate, flatten=flatten)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXHYL26YBxBH"
      },
      "source": [
        "def get_dataset(dataset, batch_size):\n",
        "    print(\"Dataset: {}\".format(dataset))\n",
        "    if dataset == \"CIFAR10\":\n",
        "      from keras.datasets import cifar10\n",
        "      (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "    elif dataset == \"MNIST\":\n",
        "      from keras.datasets import mnist\n",
        "      (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    else:\n",
        "      raise Exception(\"Unknown dataset\")\n",
        "    if len(np.shape(X_train)) < 4:\n",
        "      X_train = tf.expand_dims(X_train, -1).numpy()\n",
        "      X_test = tf.expand_dims(X_test, -1).numpy()\n",
        "\n",
        "    image_shape = np.shape(X_train)[1:]\n",
        "    print('Dataset size: {}'.format(len(X_train)))\n",
        "    print('Test size: {}'.format(len(X_test)))\n",
        "    # X_train = tf.divide(tf.cast(X_train, tf.float32), 255.0)\n",
        "    # X_test = tf.divide(tf.cast(X_test, tf.float32), 255.0)\n",
        "    X_train = tf.divide(tf.cast(X_train, tf.float32), 255.0)\n",
        "    X_test = tf.divide(tf.cast(X_test, tf.float32), 255.0)\n",
        "    n_classes = np.max(y_test) + 1\n",
        "    # n_classes = tf.cast(tf.reduce_max(y_test)+1, dtype=tf.int32)\n",
        "    y_train = tf.reshape(tf.one_hot(y_train, n_classes), (-1, n_classes))\n",
        "    y_test = tf.reshape(tf.one_hot(y_test, n_classes), (-1, n_classes))\n",
        "\n",
        "    train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(batch_size)\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test)).shuffle(len(X_train)).batch(batch_size)\n",
        "    return train_set, test_set, image_shape, n_classes"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAz0I-CvBws_"
      },
      "source": [
        "# Layer management class\n",
        "class Network:\n",
        "    def __init__(self, shape_in, architecture, s):\n",
        "        # Arguments:\n",
        "          # shape_in := list containing the dimensions of the input\n",
        "          # architecture := list of layer initialization functions\n",
        "          # s := weight initialization seed (integer)\n",
        "          \n",
        "        #Initialize all layers\n",
        "        ff_network = []\n",
        "        for layer in architecture:\n",
        "            new_layer = layer(shape_in, tf.random.set_seed(s))\n",
        "            ff_network.append(new_layer)\n",
        "            shape_in = ff_network[-1].shape_out\n",
        "        self.ff_network = ff_network\n",
        "        self.s = s\n",
        "\n",
        "        for i in ff_network:\n",
        "            print('Layer {}: {} -> {}, activation: {}, flatten: {}, dropout: {}%'.format(i.__class__.__name__, i.shape_in, i.shape_out, i.activation, i.flatten, i.dropout_rate))\n",
        "\n",
        "    # Define or change learning parameters\n",
        "    def compile(self, lr_weights=1e-0, lr_att=1e-2, exploration_rate=2e-2):\n",
        "        self.lr_w = lr_weights\n",
        "        self.lr_a = lr_att\n",
        "        self.exploration_rate = exploration_rate\n",
        "\n",
        "    # Fit network output to prediction using cross entropy loss over all output units\n",
        "    def fit(self, inputs, y_pred, batch_size):\n",
        "        output_network = self.forward_pass(inputs)\n",
        "        delta = tf.subtract(y_pred, output_network) \n",
        "        self.backward_pass(delta/ batch_size * self.lr_a)\n",
        "\n",
        "    # Attention-modulated Hebbian learning algorithm\n",
        "    @tf.function\n",
        "    def fit_attention(self, inputs, y_true):\n",
        "        # 1. Action-selection\n",
        "        batch_size = inputs.shape[0]\n",
        "        output_network = self.forward_pass(inputs)\n",
        "        y_pred = self.get_prediction(output_network, batch_size)\n",
        "        rpe, rewards, loss = self.get_loss(output_network, y_true, y_pred, batch_size)\n",
        "\n",
        "        # 2. Attentional phase\n",
        "        for epoch in range(attention_span):\n",
        "          self.fit(inputs, y_pred, batch_size)    \n",
        "\n",
        "        # 3. Weight update\n",
        "        for layer in self.ff_network[::-1]:\n",
        "          layer.weight_update(rpe)\n",
        "\n",
        "        return rewards, tf.math.reduce_mean(loss)\n",
        "    \n",
        "    def forward_pass(self, layer_out, trainingphase=True):\n",
        "        for layer in self.ff_network:\n",
        "            layer_out = layer.forward_pass(layer_out, trainingphase)\n",
        "        return tf.nn.softmax(layer_out, axis=-1)\n",
        "    \n",
        "    def backward_pass(self, feedback):\n",
        "        for layer in self.ff_network[::-1]:\n",
        "            feedback = layer.backward_pass(feedback)\n",
        "\n",
        "    def get_loss(self, output_network, y_true, y_pred, batch_size):\n",
        "        # Compute training accuracy\n",
        "        rewards = tf.math.count_nonzero(tf.multiply(y_true, y_pred), axis=1, dtype=tf.float32)\n",
        "\n",
        "        # Compute prediction error according to cross entropy loss derivative (HERE it probably meant SQE derivative) (NO the two derivatives are the same for both CE and SQE)\n",
        "        deltas = tf.subtract(rewards, tf.reduce_sum(tf.multiply(output_network, y_pred), axis=1))\n",
        "        deltas = tf.clip_by_value(deltas, -2, 4) / batch_size * self.lr_w\n",
        "\n",
        "        # Compute cross entropy loss\n",
        "        clipped_out = tf.clip_by_value(output_network, 1e-15, 1-1e-15)\n",
        "        loss = tf.reduce_sum(-tf.reduce_sum(y_true * tf.math.log(clipped_out), axis=-1)) #mean, in case of a minibatch?\n",
        "        # should it be loss = -zy (activation of the neuron corresponding to the correct class) + log(sumj exp(activation)j)\n",
        "        return tf.math.reduce_mean(deltas), tf.reduce_mean(rewards), loss\n",
        "\n",
        "    def get_prediction(self, output_network, batch_size):\n",
        "        argmax_vector = tf.argmax(output_network, axis=1)\n",
        "        random_vector = tf.random.uniform([batch_size], minval=0, maxval=1, seed=self.s)                                                                                                                                                                                                                      \n",
        "        multinomial_vector = tf.reshape(tf.random.categorical(output_network, 1, seed=self.s), [-1])\n",
        "        selected_classes = tf.where(tf.greater(random_vector, self.exploration_rate), argmax_vector, multinomial_vector)\n",
        "        return tf.one_hot(selected_classes, output_network.shape[1])\n",
        "\n",
        "    @tf.function\n",
        "    def evaluate(self, inputs, y_true):\n",
        "        fb_out = self.forward_pass(inputs, False)\n",
        "        equality = tf.equal(tf.argmax(fb_out, 1), tf.argmax(y_true, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
        "        return accuracy"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lgUyDKYFOwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299069f3-1b44-4ed6-a67e-8d7c31aa8dfe"
      },
      "source": [
        "# Retrieve data set, define validation set\n",
        "batch_size = 1 #what happens with a bigger batch? Have you tried? Basicalyy you would have to split your network, because you have to update the betas only in one direction\n",
        "train_set, test_set, input_shape, n_classes = get_dataset(\"MNIST\", batch_size)\n",
        "train_set.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "n_val= 1000//batch_size #number of batches to validate on\n",
        "validation = train_set.take(n_val)\n",
        "train = train_set.skip(n_val)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: MNIST\n",
            "Dataset size: 60000\n",
            "Test size: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkMCTj5qHn1O"
      },
      "source": [
        "def train_network(attention_span, lr_a, lr_w, w_init, s):\n",
        "    print('HEB_T{}_a{}_w{}_s{}'.format(attention_span, lr_a, lr_w, s))\n",
        "\n",
        "    network_architecture = [\n",
        "        # conv(filters=[3, 3, 32], padding='VALID', stride=[1, 1], w_std=w_init),\n",
        "        # conv(filters=[3, 3, 32], padding='SAME', stride=[2, 2],  w_std=w_init, dropout_rate=0.8),\n",
        "        dense(1500,  w_std=w_init),\n",
        "        dense(1000,  w_std=w_init),\n",
        "        dense(500,  w_std=w_init),\n",
        "        dense(n_classes,  w_std=w_init, activation=tf.identity)\n",
        "    ]\n",
        "\n",
        "    network = Network(input_shape, network_architecture, s)\n",
        "    network.compile(lr_weights=lr_w, lr_att=lr_a)\n",
        "    counter=0\n",
        "\n",
        "    old_acc=0.\n",
        "    start=time.time()\n",
        "    for epoch in range(150):  #n_epochs is not passed?\n",
        "        tic = time.time()\n",
        "        loss, rewards = 0, 0\n",
        "        for (batch, (X, Y)) in enumerate(train):\n",
        "            curr_reward, curr_loss = network.fit_attention(X, Y)\n",
        "            loss+=curr_loss\n",
        "            rewards += curr_reward\n",
        "\n",
        "        avg_acc = 0.\n",
        "        for X_test, Y_test in iter(validation):\n",
        "            curr_acc = network.evaluate(X_test, Y_test)\n",
        "            avg_acc += curr_acc \n",
        "        print('Epoch: {} Time elapsed: {:.2f}: Training loss: {:.4f} Training accuracy: {:.2f}% Validation accuracy: {:.2f}%'.format(epoch, time.time()-tic, loss/(batch+1), rewards/(batch+1) * 100, avg_acc/n_val * 100))\n",
        "    avg_acc = 0.\n",
        "    for (batch, (X_test, Y_test)) in enumerate(test_set):\n",
        "        curr_acc = network.evaluate(X_test, Y_test)\n",
        "        avg_acc += curr_acc\n",
        "    print('Test accuracy: {}'.format(avg_acc / (batch + 1)))\n",
        "    print('Time elapsed: {}'.format(time.time()-start))\n"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VbnomowHnkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a1ea72-986a-4116-b822-82244a1617c7"
      },
      "source": [
        "n_epochs = 150\n",
        "# input_shape = [32,32,3]\n",
        "# input_shape = [28,28,1]\n",
        "# n_classes = 10\n",
        "attention_span = 1  #only one attention update per weight update? Why is it so slow then (current settings approx 2.5 minutes per epoch)? Because batch ==1 probably.\n",
        "lr_attention = 5e-3\n",
        "# attention_span * lr_attention constant to have the same global update. The max was approx 20\n",
        "lr_weights = 1e-0\n",
        "weight_seed=0\n",
        "std_weights = 0.02\n",
        "\n",
        "train_network(attention_span, lr_attention, lr_weights, std_weights, weight_seed)   \n",
        "\n",
        "\n",
        "## NB QAGREL was compared with batch_size = 1. If you change the batch size you have to change also the learning rate! instead you used 1e-2, i.e. the same as I use for bs100. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HEB_T1_a0.005_w1.0_s0\n",
            "Layer Dense: [28, 28, 1] -> [1500], activation: <function relu at 0x7fc266b0d8c8>, flatten: True, dropout: 0.0%\n",
            "Layer Dense: [1500] -> [1000], activation: <function relu at 0x7fc266b0d8c8>, flatten: False, dropout: 0.0%\n",
            "Layer Dense: [1000] -> [500], activation: <function relu at 0x7fc266b0d8c8>, flatten: False, dropout: 0.0%\n",
            "Layer Dense: [500] -> [10], activation: <function identity at 0x7fc266d17b70>, flatten: False, dropout: 0.0%\n",
            "Epoch: 0 Time elapsed: 108.50: Training loss: 0.8871 Training accuracy: 68.95% Validation accuracy: 92.00%\n",
            "Epoch: 1 Time elapsed: 109.35: Training loss: 0.2020 Training accuracy: 93.02% Validation accuracy: 96.10%\n",
            "Epoch: 2 Time elapsed: 107.94: Training loss: 0.1379 Training accuracy: 94.74% Validation accuracy: 97.10%\n",
            "Epoch: 3 Time elapsed: 108.63: Training loss: 0.1084 Training accuracy: 95.62% Validation accuracy: 97.20%\n",
            "Epoch: 4 Time elapsed: 107.76: Training loss: 0.0892 Training accuracy: 96.28% Validation accuracy: 97.80%\n",
            "Epoch: 5 Time elapsed: 105.84: Training loss: 0.0747 Training accuracy: 96.66% Validation accuracy: 98.60%\n",
            "Epoch: 6 Time elapsed: 107.32: Training loss: 0.0647 Training accuracy: 96.90% Validation accuracy: 98.30%\n",
            "Epoch: 7 Time elapsed: 106.39: Training loss: 0.0570 Training accuracy: 97.21% Validation accuracy: 99.20%\n",
            "Epoch: 8 Time elapsed: 106.34: Training loss: 0.0501 Training accuracy: 97.33% Validation accuracy: 98.80%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIHY9jDNH4hP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}